{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Melanoma, Nevus and Seborrheic Keratosis with Convolutional Neural Networks\n",
    "\n",
    "In this notebook, I will tackle Disease Classification, the third challenge of the [ISIC 2017: Skin Lesion Analysis Towards Melanoma Detection](https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a). The challenge was about classifying skin lesion images into three classes: malignant melanoma (MM), nevus and seborrheic keratosis (SK). The former is the deadliest one, while nevus and seborrheic keratosis are benign forms of lesions.\n",
    "\n",
    "The first part will consist of creating a malignant melanoma classifier using transfer learning with a pretrained ResNet50. On the second part I will create a classifier for seborrheic keratosis, also using transfer learning, and use these two classifiers to classify the 3 classes.\n",
    "\n",
    "DISCLAMER: I'm not the owner of the dataset. Therefore, I don't include it along this notebook. It can be downloaded on the ISIC website of the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin lesion images dataset\n",
    "\n",
    "The provided dataset contains training 2000 images, 150 for validation purposes and 600 images for testing and submitting your model's performance.\n",
    "\n",
    "More specifically we have this distribution:\n",
    "\n",
    "- Train\n",
    "  - 374 Melanoma\n",
    "  - 1372 Nevus\n",
    "  - 254 Seborrheic keratosis\n",
    "\n",
    "- Valid\n",
    "  - 30 Melanoma\n",
    "  - 78 Nevus\n",
    "  - 42 Seborrheic keratosis\n",
    "  \n",
    "- Test\n",
    "  - 117 Melanoma\n",
    "  - 393 Nevus\n",
    "  - 90 Seborrheic keratosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def count_images(set_name):\n",
    "    print('{} {}\\t\\t {} melanoma\\t {} nevus\\t {} seborrheic keratosis'.format(\n",
    "        np.array(glob(f\"data/{set_name}/*/*\")).shape[0],\n",
    "        set_name,\n",
    "        np.array(glob(f\"data/{set_name}/melanoma/*\")).shape[0],\n",
    "        np.array(glob(f\"data/{set_name}/nevus/*\")).shape[0],\n",
    "        np.array(glob(f\"data/{set_name}/seborrheic_keratosis/*\")).shape[0],\n",
    "        ))\n",
    "\n",
    "\n",
    "count_images('train')\n",
    "count_images('valid')\n",
    "count_images('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch\n",
    "from torchvision.datasets.folder import make_dataset, has_file_allowed_extension, default_loader, IMG_EXTENSIONS\n",
    "\n",
    "\n",
    "class CustomImageFolder(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, transform=None, target_transform=None,\n",
    "                 loader=default_loader, custom_class_to_idx=None):\n",
    "        extensions = IMG_EXTENSIONS\n",
    "        \n",
    "        classes, class_to_idx = self._find_classes(root)\n",
    "        \n",
    "        if custom_class_to_idx is not None:\n",
    "            self.class_to_idx = custom_class_to_idx\n",
    "        else:\n",
    "            self.class_to_idx = class_to_idx\n",
    "        \n",
    "        samples = make_dataset(root, self.class_to_idx, extensions)\n",
    "        if len(samples) == 0:\n",
    "            raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported extensions are: \" + \",\".join(extensions)))\n",
    "\n",
    "        self.root = root\n",
    "        self.loader = loader\n",
    "        self.extensions = extensions\n",
    "\n",
    "        self.classes = classes\n",
    "        self.samples = samples\n",
    "        self.targets = [s[1] for s in samples]\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.imgs = self.samples\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset.\n",
    "        Args:\n",
    "            dir (string): Root directory path.\n",
    "        Returns:\n",
    "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "        Ensures:\n",
    "            No class is a subdirectory of another.\n",
    "        \"\"\"\n",
    "        if sys.version_info >= (3, 5):\n",
    "            # Faster and available in Python 3.5 and above\n",
    "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target, path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "img_size = 224\n",
    "\n",
    "transforms = {\n",
    "    'training': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size),\n",
    "#         transforms.RandomRotation(10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                             std = [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'testing': transforms.Compose([\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                             std = [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 20\n",
    "num_cores = 8\n",
    "class_to_idx = {'melanoma': 1, 'nevus': 0, 'seborrheic_keratosis': 0}\n",
    "\n",
    "data_loaders = {\n",
    "    'train': DataLoader(\n",
    "        CustomImageFolder(\"data/train/\",\n",
    "                          transform = transforms['training'],\n",
    "                          custom_class_to_idx = class_to_idx),\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = num_cores),\n",
    "    'valid': DataLoader(\n",
    "        CustomImageFolder(\"data/valid/\",\n",
    "                          transform = transforms['testing'],\n",
    "                          custom_class_to_idx = class_to_idx),\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_cores),\n",
    "    'test': DataLoader(\n",
    "        CustomImageFolder(\"data/test/\",\n",
    "                          transform = transforms['testing'],\n",
    "                          custom_class_to_idx = class_to_idx),\n",
    "        batch_size = batch_size, num_workers = num_cores)   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding hyperparameters\n",
    "\n",
    "In order to augment the data, we will perform transformations of the images (resizing, cropping, rotations, flipping). This step will allow for the network to generalize better, in addition of providing more samples by tweaking the ones we possess.\n",
    "\n",
    "- Resizing: this parameter is an important one. It will determine the number of units our classifier will have. As a result, it cannot be changes after the network is trained, or we will have to redefine the arquitecture and train from the beginning.\n",
    "- Random rotation: As far as we are training to classify skin lessions and not cats and dogs, the orientation of the image does not actually matter. Therefore, a random rotation up to 359 degrees is defined.\n",
    "\n",
    "The normalizing transform is necessary because we are using a ResNet50 for transfer learning. The images used for training this CNN were normalized using those parameters and therefore, it is necessary to apply them to get the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be run multiple times to get a sense of the image sizes in the dataset. When choosing a resize value for the transform, we want a value that is enough for the network to pick up the details but not so high that it takes too much time to train and low resolution images are upscaled too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sense of the images size by running this cell multiple times\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "files = np.array(glob('data/train/*/*'))\n",
    "file = files[np.random.randint(0, files.shape[0])]\n",
    "image = Image.open(file)\n",
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- img_size: After running the cell above several times, I decide a size of 512 pixels should be enough.\n",
    "- num_cores: this parameter is used by the transformers and should be less or equal to the number of cores of your CPU for best performance.\n",
    "- batch_size: the batch_size will depend on the memory you have available and how much of a bottleneck your CPU is to your GPU. 20 works well for me and is enough for the network to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining data loaders\n",
    "\n",
    "Several things are going on in the cell below so I will explain part by part.\n",
    "\n",
    "First of all, we define the transform pipelines. There are two. One for training, with transformations to augment the data, and one for validating and testing, which does not perform any transformation. The former will augment the images by cropping, rotating and flipping them, while the latter only has the mandatory transformations for feeding the image to the network: resizing to a fixed size, transforming the PIL Image to a PyTorch tensor and noramlizing with the values of ResNet50.\n",
    "\n",
    "After this step, we are ready to load the data.\n",
    "\n",
    "I use the ImageFolder class from pytorch's torchvision.datasets module to import the images. ImageFolder is very handy, because it labels the images based on the name of the folder the are in. However, this is not exactly what I need. As far as I am tryining to classify malignant melanoma vs the rest, I don't need two separate labels for nevus and SK. Instead, what I need is to label them as a single class \"rest\". As a result, I import the function *make_dataset* from torchvision module and I provide the mapping of classes that fits to my goal.\n",
    "\n",
    "It is worth mentioning a new requirement for the testing dataloader. Here, a new class is defined. The submitting format of the CSV of predictions demands to include the path of the image. The ImageFolder class returns the image as a tensor and the label. However, it does not return the path to the image. To solve this problem I am defining a new class that inherits from ImageFolder, i.e. that has all the functionality of the ImageFolder class. After that, I redefine the method that implements the population of items to provide the behaviour that I want, returning not only the image and the label, but also its path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if a GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "cuda_is_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I download the pretrained ResNet50 model. As far as we are applying transfer learning, I freeze the weights of the convolutional network and redefine the classifier to output only two classes: malignant melanoma or rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ResNet152\n",
    "MM_model = models.resnet152(pretrained=True)\n",
    "for param in MM_model.parameters():\n",
    "    param.requires_grad = False\n",
    "MM_model.fc = nn.Sequential(\n",
    "    nn.Linear(MM_model.fc.in_features, 2)\n",
    "  )\n",
    "\n",
    "# ResNet50\n",
    "# MM_model = models.resnet50(pretrained=True)\n",
    "# # for param in MM_model.parameters():\n",
    "# #     param.requires_grad = False\n",
    "# MM_model.fc = nn.Sequential(\n",
    "#     nn.Linear(MM_model.fc.in_features, 2)\n",
    "#   )\n",
    "\n",
    "# ResNet18\n",
    "# MM_model = models.resnet18(pretrained=True)\n",
    "# for param in MM_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# # for param in MM_model.layer4.parameters():\n",
    "# #     requires_grad = True\n",
    "# MM_model.fc = nn.Linear(MM_model.fc.in_features, 2)\n",
    "\n",
    "# Move to GPU if available\n",
    "if cuda_is_available:\n",
    "    MM_model = MM_model.cuda()\n",
    "    \n",
    "MM_model_filename = 'models/MM_model_resnet152.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a CrossEntropyLoss criterion for this classification task and an Adam optimizer for its adaptative properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "try:\n",
    "    MM_model.load_state_dict(torch.load(MM_model_filename, map_location=torch.device('cpu')))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "modelParameters = [\n",
    "    {'params': MM_model.fc.parameters()},\n",
    "]\n",
    "\n",
    "optimizer = optim.Adam(modelParameters, lr=1e-7)\n",
    "# optimizer = optim.Adadelta(modelParameters, lr=0.01)\n",
    "# optimizer = optim.Adagrad(modelParameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training and testing function have been moved to an external python file to keep the notebook more clean. Feel free to take a look at how they work.\n",
    "\n",
    "Here I train the model for a number of epochs and load the model that got the best validation accuracy at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_functions import train\n",
    "\n",
    "epochs = 100\n",
    "train(epochs, data_loaders, MM_model, optimizer, criterion, cuda_is_available, MM_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_functions import test_to_csv\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "try:\n",
    "    MM_model.load_state_dict(torch.load(MM_model_filename, map_location=torch.device('cpu')))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "test_to_csv(data_loaders, MM_model, None, cuda_is_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "def plot_roc_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function plots the ROC curves and provides the scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize dictionaries and array\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = np.zeros(3)\n",
    "    \n",
    "    # prepare for figure\n",
    "    plt.figure()\n",
    "    colors = ['aqua', 'cornflowerblue']\n",
    "\n",
    "    # for both classification tasks (categories 1 and 2)\n",
    "    for i in range(2):\n",
    "        # obtain ROC curve\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:,i], y_pred[:,i])\n",
    "        # obtain ROC AUC\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # plot ROC curve\n",
    "        plt.plot(fpr[i], tpr[i], color=colors[i], lw=2,\n",
    "                 label='ROC curve for task {d} (area = {f:.2f})'.format(d=i+1, f=roc_auc[i]))\n",
    "    # get score for category 3\n",
    "    roc_auc[2] = np.average(roc_auc[:2])\n",
    "    \n",
    "    # format figure\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # print scores\n",
    "    for i in range(3):\n",
    "        print('Category {d} Score: {f:.3f}'. format(d=i+1, f=roc_auc[i]))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, thresh, classes):\n",
    "    \"\"\"\n",
    "    This function plots the (normalized) confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain class predictions from probabilities\n",
    "    y_pred = (y_pred>=thresh)*1\n",
    "    # obtain (unnormalized) confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # normalize confusion matrix\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "thresh = 0.5\n",
    "\n",
    "truth = pd.read_csv('ground_truth.csv')\n",
    "y_true = truth.as_matrix(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "# get model predictions for test dataset\n",
    "y_pred = pd.read_csv('predictions.csv')\n",
    "y_pred = y_pred.as_matrix(columns=[\"task_1\", \"task_2\"])\n",
    "\n",
    "# plot ROC curves and print scores\n",
    "plot_roc_auc(y_true, y_pred)\n",
    "# plot confusion matrix\n",
    "classes = ['benign', 'malignant']\n",
    "plot_confusion_matrix(y_true[:,0], y_pred[:,0], thresh, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
